# -*- coding: utf-8 -*-
"""Trust Region Newton CG Method.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FuHMNyWDnZUUgejV8kxm2RHa1FV5EWOZ
"""

from google.colab import drive
drive.mount('/content/drive')
import sys
sys.path.append('/content/drive/MyDrive')

import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats
import scipy.sparse as sparse
import scipy.io

##Define Quadratic method for dimension 10 and Kappa 10
## Cited from the project information of IOE691
rng = np.random.RandomState(0)
x0 = 20 * rng.random((10, 1)) - 10
def quad_10_10_func(x):
    # set raondom seed
    np.random.seed(0)
    # Generate random data
    q = np.random.normal(size=(10,1))

    mat = scipy.io.loadmat(path+'quad_10_10_Q.mat')
    Q = mat['Q']
    # compute function value
    return (1/2*x.T@Q@x + q.T@x)[0]

def quad_10_10_grad(x):
    # set raondom seed
    np.random.seed(12)
    # Generate random data
    q = np.random.normal(size=(10,1))
    mat = scipy.io.loadmat(path+'quad_10_10_Q.mat')
    Q = mat['Q']

    return Q@x + q

def quad_10_10_Hess(x):
    # set raondom seed
    np.random.seed(12)
    # Generate random data
    q = np.random.normal(size=(10,1))
    mat = scipy.io.loadmat(path+'quad_10_10_Q.mat')
    Q = mat['Q']
    I=np.identity(10)
    return Q@I

##Define Quadratic method for dimension 10 and Kappa 1000
def quad_10_1000_func(x):
    # set raondom seed
    np.random.seed(0)
    # Generate random data
    q = np.random.normal(size=(10,1))

    mat = scipy.io.loadmat(path+'quad_10_1000_Q.mat')
    Q = mat['Q']

    # compute function value
    return (1/2*x.T@Q@x + q.T@x)[0]

def quad_10_1000_grad(x):
    # set raondom seed
    np.random.seed(0)
    # Generate random data
    q = np.random.normal(size=(10,1))
    mat = scipy.io.loadmat(path+'quad_10_1000_Q.mat')
    Q = mat['Q']

    return Q@x + q


def quad_10_1000_Hess(x):
    # set raondom seed
    np.random.seed(0)
    # Generate random data
    q = np.random.normal(size=(10,1))
    mat = scipy.io.loadmat(path+'quad_10_1000_Q.mat')
    Q = mat['Q']
    I=np.identity(10)
    return Q@I

##Define Quadratic method for dimension 1000 and Kappa 10
def quad_1000_10_func(x):
    # set raondom seed
    np.random.seed(0)
    # Generate random data
    q = np.random.normal(size=(1000,1))

    mat = scipy.io.loadmat(path+'quad_1000_10_Q.mat')
    Q = mat['Q']

    # compute function value
    return (1/2*x.T@Q@x + q.T@x)[0]

def quad_1000_10_grad(x):
    # set raondom seed
    np.random.seed(0)
    # Generate random data
    q = np.random.normal(size=(1000,1))
    mat = scipy.io.loadmat(path+'quad_1000_10_Q.mat')
    Q = mat['Q']
    return Q@x + q


def quad_1000_10_Hess(x):
    # set raondom seed
    np.random.seed(0)
    # Generate random data
    q = np.random.normal(size=(1000,1))
    mat = scipy.io.loadmat(path+'quad_1000_10_Q.mat')
    Q = mat['Q']
    I=np.identity(1000)
    return Q@I

##Define Quadratic method for dimension 1000 and Kappa 1000
def quad_1000_1000_func(x):
    # set raondom seed
    np.random.seed(0)
    # Generate random data
    q = np.random.normal(size=(1000,1))

    mat = scipy.io.loadmat(path+'quad_1000_1000_Q.mat')
    Q = mat['Q']

    # compute function value
    return (1/2*x.T@Q@x + q.T@x)[0]

def quad_1000_1000_grad(x):
    # set raondom seed
    np.random.seed(0)
    # Generate random data
    q = np.random.normal(size=(1000,1))
    mat = scipy.io.loadmat(path+'quad_1000_1000_Q.mat')
    Q = mat['Q']
    return Q@x + q


def quad_1000_1000_Hess(x):
    # set raondom seed
    np.random.seed(0)
    # Generate random data
    q = np.random.normal(size=(1000,1))
    mat = scipy.io.loadmat(path+'quad_1000_1000_Q.mat')
    Q = mat['Q']
    I=np.identity(1000)
    return Q@I

##Trust Region Newton CG method
import numpy as np
from numpy import linalg
def trust_region(x0, deltahat, delta0, eta, f, g, h, max_iter=100, method="cauchy", tol=1e-12):
    assert deltahat > 0
    assert eta >= 0 and eta < 1/4
    assert delta0 > 0 and delta0 < deltahat
    delta = delta0
    n = len(x0)
    xvals = [x0]
    ncalls = 0
    for i in range(1, max_iter+1):
        if i % 100 == 0:
            print("Iteration: ", i)
        xcurr = xvals[-1]
        fk = f(xcurr)
        gk = g(xcurr)
        Bk = h(xcurr)
        if not check_posdef(Bk):
            Bk = cholesky_mod(1e-3, Bk)
            ncalls += 1
        p = solve_subproblem(delta, gk, Bk, method)
        rho = compute_rho(xcurr, f, gk, Bk, p)
        # decrease trust region radius
        if rho < 1/4:
            delta *= 1/4
        else:
            # increase trust region radius
            if rho > 3/4 and np.linalg.norm(p,2) == delta:
                delta = min(2*delta, deltahat)
        # accept the step
        if rho > eta:
            xnew = xcurr + p
        # reject the step
        else:
            xnew = xcurr
        xvals.append(xnew)
        if abs(np.mean(xnew - xcurr)) <= tol:
            print("Number of indefinite fixes ", ncalls)
            print("Number of iterations: ", i)
            return xvals
    print("Finished algorithm without converging.")
    return xvals

def model(p, xcurr, fk, gk, Bk):
    #p: vector in which to step in
    #xcurr: vector of the current iterate
    #fk: scalar, f(xk)
    #gk: vector, gradient(xk)
    #Bk: symmetric matrix, e.g., Hessian(xk)
    return fk + np.dot(gk.T, p) + 1/2 * np.dot(np.dot(p.T, Bk), p)

def compute_rho(xcurr, f, gk, Bk, p):
    #xcurr: vector of the current iterate
    #f: the objective function
    #gk: gradient(xk)
    #Bk: symmetric matrix
    #p: step
    fk = f(xcurr)
    n = len(p)
    return ((fk - f(xcurr + p)) / (model(np.zeros(n), xcurr, fk, gk, Bk) - model(p, xcurr, fk, gk, Bk)))

def cholesky_mod(beta, H, max_iter=1000):
    assert beta > 0
    if np.min(np.diag(H)) > 0:
        tau = 0
    else:
        tau = -np.min(np.diag(H)) + beta
    for i in range(max_iter):
        candidate = H + tau * np.eye(H.shape[0])
        if check_posdef(candidate):
            return candidate
        tau = max(2*tau, beta)
    raise Exception("Infinite loop encountered")

def check_posdef(A):
    try:
        L = np.linalg.cholesky(A)
        return True
    except:
        return False

def solve_subproblem(delta, gk, Bk, method="cauchy"):
    #Solves the trust-region subproblem, returning a search direction pk.
    #delta: trust-region radius
    #gk: gradient at current iterate
    #Bk: Hessian (or symmetric PD matrix) at current iterate
    #"cg_steihaug" - Steihaug algorithm, CG method
    if  method == "cg_steihaug":
        return cg_steihaug(delta, gk, Bk)
    else:
        print("defaulting to Cauchy point")
        return cauchy_point(delta, gk, Bk)

def cauchy_point(delta, gk, Bk):

    #delta: positive scalar, trust-region radius at current iteration
    #gk: gradient at current iterate
    #Bk: symmetric matrix, e.g., Hessian

    tau = 1
    if np.dot(gk.T, np.dot(Bk, gk)) > 0:
        tau = min(np.linalg.norm(gk, 2)**3 / (delta * np.dot(gk.T, np.dot(Bk, gk))), 1)
    # Cauchy point equation
    return -tau * delta / np.linalg.norm(gk, 2) * gk

def cg_steihaug(delta, gk, Bk):
    gradnorm = np.linalg.norm(gk, 2)
    tol = min(0.5, np.sqrt(gradnorm)) * gradnorm
    z = np.zeros_like(gk)
    r = gk
    d = -gk
    if np.linalg.norm(r, 2) < tol:
        return z
    while True:
        # negative curvature
        if np.dot(np.dot(d.T, Bk), d) <= 0:
            return cauchy_point(delta, gk, Bk)
        a = (np.dot(r.T, r) / np.dot(np.dot(d.T, Bk), d))
        z += a * d
        # violates trust region bound
        if np.linalg.norm(z, 2) >= delta:
            return cauchy_point(delta, gk, Bk)
        rold = r
        r += a * np.dot(Bk, d)
        # convergence tolerance satisfied
        if np.linalg.norm(r, 2) <= tol:
            return z
        beta = (np.dot(r.T, r) / np.dot(rold.T, rold))
        d = -r + beta * d

# Run the Trust Region Newton Method in q10_10
rng = np.random.RandomState(0)
x0 = 20 * rng.random((10, 1)) - 10
cvals=trust_region(x0, 3, 1, 0.1, quad_10_10_func,quad_10_10_grad,quad_10_10_Hess, 2000, "cg_steihaug",1e-12);

#Plot the function value
nsamps = len(cvals)
fx = [quad_10_10_func(cvals[i]) for i in range(nsamps)]
grads = [np.linalg.norm(quad_10_10_grad(cvals[i]), 2) for i in range(nsamps)]
plt.plot(range(1, nsamps + 1), fx, color="red")
plt.scatter(range(1, nsamps + 1), fx,color="red")
plt.xlabel("Iterations")
plt.ylabel("Function Value")
plt.show()

# plot the norm of gradient
plt.plot(range(1, nsamps + 1), grads, color="red")
plt.xlabel("Iterations")
plt.ylabel("Norm of Gradient")
plt.scatter(range(1, nsamps + 1), grads, color="red")
plt.show()

# Run the Trust Region Newton Method in q10_10
rng = np.random.RandomState(0)
cvals=trust_region(x0,3, 0.1,0.1, quad_10_1000_func,quad_10_1000_grad,quad_10_1000_Hess, 2000, "cg_steihaug",1e-12);

# plot the function value
nsamps = len(cvals)
fx = [quad_10_1000_func(cvals[i]) for i in range(nsamps)]
grads = [np.linalg.norm(quad_10_1000_grad(cvals[i]), 2) for i in range(nsamps)]
plt.plot(range(1, nsamps + 1), fx, color="red")
plt.scatter(range(1, nsamps + 1), fx,color="green")
plt.xlabel("Iterations")
plt.ylabel("Function Value")
plt.show()

# plot the norm of gradient
plt.plot(range(1, nsamps + 1), grads, color="red")
plt.xlabel("Iterations")
plt.ylabel("Norm of Gradient")
plt.scatter(range(1, nsamps + 1), grads, color="green")
plt.show()

## Run the Trust Region Newton Method in q1000_10
rng = np.random.RandomState(0)
x0 = 20 * rng.random((1000, 1)) - 10
cvals=trust_region(x0,3, 1, 0.1, quad_1000_10_func,quad_1000_10_grad,quad_1000_10_Hess, 2000, "cg_steihaug",1e-12);

#plot the function value
nsamps = len(cvals)
fx = [quad_1000_10_func(cvals[i]) for i in range(nsamps)]
grads = [np.linalg.norm(quad_1000_10_grad(cvals[i]), 2) for i in range(nsamps)]
plt.plot(range(1, nsamps + 1), fx, color="red")
plt.scatter(range(1, nsamps + 1), fx,color="green")
plt.xlabel("Iterations")
plt.ylabel("Function Value")
plt.show()

#plot the norm of gradient
plt.plot(range(1, nsamps + 1), grads, color="red")
plt.xlabel("Iterations")
plt.ylabel("Norm of Gradient")
plt.scatter(range(1, nsamps + 1), grads, color="green")
plt.show()

# Run the Trust Region Newton Method in q1000_1000
cvals=trust_region(x0,3, 1,0.1, quad_1000_1000_func,quad_1000_1000_grad,quad_1000_1000_Hess, 2000, "cg_steihaug",1e-12);

# Plot the function value
nsamps = len(cvals)
fx = [quad_1000_1000_func(cvals[i]) for i in range(nsamps)]
grads = [np.linalg.norm(quad_1000_1000_grad(cvals[i]), 2) for i in range(nsamps)]
plt.plot(range(1, nsamps + 1), fx, color="red")
plt.scatter(range(1, nsamps + 1), fx,color="green")
plt.xlabel("Iterations")
plt.ylabel("Function Value")
plt.show()

# plot the norm of gradient
plt.plot(range(1, nsamps + 1), grads, color="red")
plt.xlabel("Iterations")
plt.ylabel("Norm of Gradient")
plt.scatter(range(1, nsamps + 1), grads, color="green")
plt.show()

